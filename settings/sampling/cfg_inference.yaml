# @package _global_

defaults:
  - _self_

task_name: "cfg_inference" 
tags: ["dev"]

data:
  _target_: src.data.full_atom.datamodule.FullAtomDataModule
  
  diffuser: ${model.diffuser}
  train_batch_size: 32
  gen_batch_size: 75
  val_batch_size: 32
  num_workers: 0
  pin_memory: True
  clustering_training: True
  dynamic_batching: False

  repr_loader:
    _target_: src.data.full_atom.feat_loader.ESMFoldReprLoader
    # Use interpolated variable path
    data_root: ${paths.esm.repr_dir}
    node_size: 1024
    edge_size: 128

  dataset:
    train_dataset:
      _target_: src.data.guidance.dataset.GuidanceDataset
      csv_path: null
      data_dir: null
 
    val_dataset:
      _target_: src.data.guidance.dataset.GuidanceDataset
      csv_path: null
      data_dir: null
  
    val_gen_dataset: null

    test_gen_dataset:
      _target_: src.data.full_atom.dataset.GenDataset
      csv_path: null
      num_samples: 1

model:
  _target_: src.models.full_atom.module.FullAtomLitModule

  optimizer: ${optimizer}
  scheduler: ${scheduler}

  stage: ??? 
  
  lr_warmup_steps: 5000
  val_gen_every_n_epochs: 100
  output_dir: ${paths.output_dir}
  log_loss_name: ["total"]

  diffuser:
    _target_: src.models.full_atom.diffuser.se3_diffuser.SE3Diffuser
    se3_conf:
      diffuse_trans: true
      diffuse_rot: true
      r3:
        min_b: 0.1
        max_b: 20.0
        coordinate_scaling: 0.1 
      so3:
        num_omega: 1000
        num_sigma: 1000
        min_sigma: 0.1
        max_sigma: 1.5
        schedule: "logarithmic"
        cache_dir: ".cache/"
        use_cached_score: False

  score_network:
    _target_: src.models.guidance.score_network.GuidanceScoreNetwork
    # Use interpolated variable path for MSTA
    msta_dir: ${paths.rcsb.msta_dir}
    
    # Use interpolated variable paths for Checkpoints
    cond_ckpt_path: ${paths.guidance.cond_ckpt}
    uncond_ckpt_path: ${paths.guidance.uncond_ckpt}
    
    cfg:
      rot_loss_weight: 0.5
      rot_angle_loss_t_filter: 0.2
      trans_loss_weight: 1.0
      bb_coords_loss_weight: 0.2
      bb_coords_loss_t_filter: 0.2
      bb_dist_map_loss_weight: 0.2
      bb_dist_map_loss_t_filter: 0.2
      torsion_loss_weight: 0.5
      fape_loss_weight: 1.0
      num_samples: 10
      inf_batch_size: 10
      scale_coords: 0.1 
      stage1_diffusion_steps: 300
      stage2_diffusion_steps: 100
      base_steps: 100
      starting_steps: 0
      final_steps: 300
      folding_steps: 300
      clsfree_guidance_strength: 0.95

    cond_model_nn:
      _target_: src.models.full_atom.model_nn.base.FoldNet
      embedder:
        _target_: src.models.full_atom.model_nn.embedder.Embedder
        time_emb_size: 64
        scale_t: 1000. 
        res_idx_emb_size: 64
        num_rbf: 64
        rbf_min: 0.
        rbf_max: 5.
        pretrained_node_repr_size: ${data.repr_loader.node_size}
        pretrained_edge_repr_size: ${data.repr_loader.edge_size}
        node_emb_size: 256
        edge_emb_size: 128
      structure_module:
        _target_: src.models.full_atom.model_nn.structure_module.StructureModule
        num_ipa_blocks: 6
        c_s: 256
        c_z: 128
        c_hidden: 256
        c_skip: 64
        no_heads: 6
        no_qk_points: 8
        no_v_points: 12
        seq_tfmr_num_heads: 8
        seq_tfmr_num_layers: 2
    
    uncond_model_nn:
      _target_: src.models.full_atom.model_nn.base.FoldNet
      embedder:
        _target_: src.models.full_atom.model_nn.embedder.Embedder
        time_emb_size: 64
        scale_t: 1000. 
        res_idx_emb_size: 64
        num_rbf: 64
        rbf_min: 0.
        rbf_max: 5.
        pretrained_node_repr_size: 0
        pretrained_edge_repr_size: 0
        node_emb_size: 256
        edge_emb_size: 128
      structure_module:
        _target_: src.models.full_atom.model_nn.structure_module.StructureModule
        num_ipa_blocks: 6
        c_s: 256
        c_z: 128
        c_hidden: 256
        c_skip: 64
        no_heads: 6
        no_qk_points: 8
        no_v_points: 12
        seq_tfmr_num_heads: 8
        seq_tfmr_num_layers: 2

trainer:
  strategy: ddp_find_unused_parameters_true
  precision: 32
  max_epochs: 1000
  use_distributed_sampler: True
  inference_mode: True
  enable_progress_bar: False

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  lr: 3e-4
  weight_decay: 0.0

scheduler:
  _target_: torch.optim.lr_scheduler.ReduceLROnPlateau
  _partial_: true
  mode: min
  factor: 0.5
  patience: 10
  threshold: 0.001
  min_lr: 1e-6

callbacks:
  model_checkpoint_by_epoch:
    every_n_epochs: 10
  early_stopping: null
  model_summary: null
  rich_progress_bar: null

seed: null
ckpt_path: null